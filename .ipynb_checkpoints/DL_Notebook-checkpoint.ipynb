{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amazing-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "import matplotlib.pyplot as mpl\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kayde\\OneDrive\\Desktop\\mannada\\RainfallData\n"
     ]
    }
   ],
   "source": [
    "#Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sapphire-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the variables as recorded by the station\n",
    "variables = [\n",
    "        'dswr',\n",
    "        'lftx',\n",
    "        'mslp',\n",
    "        'p__f',\n",
    "        'p__u',\n",
    "        'p__v',\n",
    "        'p__z',\n",
    "        'p_th',\n",
    "        'p_zh',\n",
    "        'p5_f',\n",
    "        'p5_u',\n",
    "        'p5_v',\n",
    "        'p5_z',\n",
    "        'p5th',\n",
    "        'p5zh',\n",
    "        'p8_f',\n",
    "        'p8_u',\n",
    "        'p8_v',\n",
    "        'p8_z',\n",
    "        'p8th',\n",
    "        'p8zh',\n",
    "        'p500',\n",
    "        'p850',\n",
    "        'pottmp',\n",
    "        'pr_wtr',\n",
    "        'prec',\n",
    "        'r500',\n",
    "        'r850',\n",
    "        'rhum',\n",
    "        'shum',\n",
    "        'temp',\n",
    "]\n",
    "\n",
    "#The three regions\n",
    "regions = [\n",
    "        '82.5', #A\n",
    "        '85.0', #B\n",
    "        '87.5', #C\n",
    "]\n",
    "\n",
    "'''\n",
    "TAKING IN ALL THE X VARIABLES\n",
    "'''\n",
    "\n",
    "#define an empty dictionary\n",
    "boxes = {}\n",
    "\n",
    "#iterating over regions\n",
    "for region in regions:\n",
    "    \n",
    "    df_li = pd.DataFrame() # empty dataframe\n",
    "    \n",
    "    #iterating over variables\n",
    "    for var in variables:\n",
    "        \n",
    "        path = os.path.join(cwd,\"DATA\\\\BOX_20N_%sE\\\\ncep_%s.dat\"%(region,var)) # Cleverly curated path for automation\n",
    "        \n",
    "        with open(path) as file: #open the respected variable file for the region\n",
    "            \n",
    "            temp = file.read().splitlines()\n",
    "            temp = [float(i) for i in temp]\n",
    "            \n",
    "        df_li[region + '_' + var] = temp #and for every variable, store that data under header: <coordinate_variable>\n",
    "        \n",
    "    boxes[region]=df_li #and assign that dataframe for every region\n",
    "    \n",
    "'''\n",
    "TAKING IN ALL THE Y VALUES\n",
    "'''\n",
    "\n",
    "rain = []\n",
    "\n",
    "for i in range(5): # iterate from 0 to 4 i.e. 5 times\n",
    "    \n",
    "    path = os.path.join(cwd,\"DATA\\\\rain%d.dat\"%(i+1)) # join cwd to path of rain data\n",
    "    \n",
    "    with open(path) as file:\n",
    "        \n",
    "        lines = file.read().splitlines()\n",
    "        lines = [float(i) for i in lines]\n",
    "        rain.append( lines )  #store in rain list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "searching-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time for some data refining and model training\n",
    "\n",
    "#Trimming BOXES, this is to match with the rain data, day-by-day\n",
    "for region in regions:\n",
    "    boxes[region] = boxes[region].iloc[4749:,] # removing 4749 rows from front\n",
    "    boxes[region] = boxes[region].iloc[:10957,] # keeping only 10957 of the rest\n",
    "\n",
    "#sample take BOX A and PLACE 1\n",
    "df = boxes['82.5']\n",
    "rain1 = rain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indoor-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictionModel:\n",
    "    '''\n",
    "    This is a class defining the Prediction Neural Network, Its input filtering,\n",
    "    '''\n",
    "    def __init__(self,X,Y,classStr,N=1,K=31):\n",
    "        if classStr == 'regressor':\n",
    "            self.__model = self.__neuralNetworkRegressor(N,K)\n",
    "        elif classStr == 'classifier':\n",
    "            self.__model = self.__neuralNetworkClassifier(N,K)\n",
    "        else:\n",
    "            print('\\nError: model %s not found'%(classStr))\n",
    "        self.__K = K\n",
    "        self.__N = N\n",
    "        self.__xdata = X\n",
    "        self.__ydata = Y\n",
    "        self.__class = classStr\n",
    "    def __neuralNetworkRegressor(self,N,K):\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Dense(\n",
    "                    31*N,\n",
    "                    input_dim=31*N,\n",
    "                    activation='sigmoid', #next layer is relu, so it needs normalization\n",
    "                    kernel_initializer=initializers.GlorotNormal(seed=None), #glorot works good with sigmoid\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "                Dense(\n",
    "                    K,\n",
    "                    activation='relu', #relu, for better regression\n",
    "                    kernel_initializer=initializers.HeNormal(seed=None), #He Normal goes good with relu\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "                Dense(\n",
    "                    1,\n",
    "                    kernel_initializer='normal',\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy','mse'])\n",
    "        return model\n",
    "    \n",
    "    def __neuralNetworkClassifier(self,N,K):\n",
    "        \n",
    "        model = Sequential(\n",
    "            [\n",
    "                Dense(\n",
    "                    31*N,\n",
    "                    input_dim=31*N,\n",
    "                    activation='relu',\n",
    "                    kernel_initializer=initializers.HeNormal(seed=None), #He Normal goes good with relu\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "                Dense(\n",
    "                    K,\n",
    "                    activation='relu', \n",
    "                    kernel_initializer=initializers.HeNormal(seed=None), #He Normal goes good with relu\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "                Dense(\n",
    "                    1,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='normal',\n",
    "                    bias_initializer='zeros',\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy','mse'])\n",
    "        return model\n",
    "    \n",
    "    def trainNetwork(self):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(self.__xdata,self.__ydata,test_size=0.2)#splitting test-train\n",
    "        print(\"Initiating Training Sequence\\n\")\n",
    "        history = self.__model.fit(\n",
    "            xtrain,\n",
    "            ytrain,\n",
    "            epochs = 10,\n",
    "            batch_size = 5,\n",
    "        )\n",
    "        print(\"\\n\\nInitiating Testing Sequence\\n\")\n",
    "        metrics = self.__model.evaluate(\n",
    "            xtest,\n",
    "            ytest,\n",
    "            batch_size = 5,\n",
    "        )\n",
    "    def evaluateNetwork(self):\n",
    "        '''\n",
    "        This Function isn't really working now,\n",
    "        Will come back later\n",
    "        '''\n",
    "        print(\"\\nInitiating Evaluation Sequence\\n\")\n",
    "        evaluators=[\n",
    "            ('standardize',StandardScaler()),\n",
    "            ('mlp',KerasRegressor(\n",
    "                build_fn =  self.__neuralNetwork(self.__N,self.__K),\n",
    "                epochs = 10,\n",
    "                batch_size = 5,\n",
    "                verbose = 0,\n",
    "            )),\n",
    "        ]\n",
    "        pipeline = Pipeline(evaluators)\n",
    "        results = cross_val_score(\n",
    "            pipeline,\n",
    "            self.__xdata,\n",
    "            self.__ydata,\n",
    "            cv = KFold(n_splits=10)\n",
    "        )\n",
    "        print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romantic-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Training Sequence\n",
      "\n",
      "Epoch 1/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 65.0084 - accuracy: 0.1268 - mse: 65.0084\n",
      "Epoch 2/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 56.6327 - accuracy: 0.3775 - mse: 56.6327\n",
      "Epoch 3/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 56.1866 - accuracy: 0.3877 - mse: 56.1866\n",
      "Epoch 4/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 55.0586 - accuracy: 0.4133 - mse: 55.0586\n",
      "Epoch 5/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 55.0953 - accuracy: 0.3806 - mse: 55.0953\n",
      "Epoch 6/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 54.7024 - accuracy: 0.4050 - mse: 54.7024\n",
      "Epoch 7/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 54.4547 - accuracy: 0.3888 - mse: 54.4547\n",
      "Epoch 8/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 54.7087 - accuracy: 0.3465 - mse: 54.7087\n",
      "Epoch 9/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 54.4934 - accuracy: 0.3619 - mse: 54.4934\n",
      "Epoch 10/10\n",
      "1753/1753 [==============================] - 2s 1ms/step - loss: 54.1194 - accuracy: 0.3829 - mse: 54.1194\n",
      "\n",
      "\n",
      "Initiating Testing Sequence\n",
      "\n",
      "439/439 [==============================] - 0s 1ms/step - loss: 52.4239 - accuracy: 0.4215 - mse: 52.4239\n"
     ]
    }
   ],
   "source": [
    "ydat = rain1.copy()\n",
    "xdat = df.values.tolist()\n",
    "rainFallPredictor = predictionModel(xdat,ydat,'regressor',1,10)\n",
    "rainFallPredictor.trainNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defined-sight",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rain'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0d8f45f96545>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrain1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mydata_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rain'"
     ]
    }
   ],
   "source": [
    "df_2 = df.copy()\n",
    "df_2['rain'] = rain1\n",
    "df_2.drop(df_2[df_2['rain'] == 0].index, inplace = True)\n",
    "ydata_2 = df_2['rain'].values.tolist()\n",
    "df_2 = df_2.drop(['rain'],axis=1)\n",
    "xdata_2=df_2.values.tolist()\n",
    "rainFallPredictor2 = predictionModel(xdata_2,ydata2,'regressor',1,400)\n",
    "rainFallPredictor2.trainNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata_3 = rain1\n",
    "for i in range(len(ydata1)):\n",
    "    if ydata_3[i] > 0:\n",
    "        ydata_3[i] = 1;\n",
    "xdata_3 = df.values.tolist()\n",
    "rainFallPredictor3 = predictionModel(xdata_3,ydata_3,'classifier',1,200)\n",
    "rainFallPredictor3.trainNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-insured",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-illustration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-muslim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-nepal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
